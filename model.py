import torch
import torch.nn as nn
import timm
import numpy as np
from typing import List, Dict, Any
import gc


class Model(nn.Module):
    """ì•™ìƒë¸” ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        num_classes: int,
        model_name: str,
        pretrained: bool = True,
        dropout_rate: float = 0.2,
        img_size: int = 384,
    ):
        super().__init__()

        # TIMM ëª¨ë¸ ìƒì„±
        self.backbone = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=0,  # feature extractorë¡œ ì‚¬ìš©
            global_pool="",
        )

        # Feature dimension ì•ˆì „í•œ ê³„ì‚°
        self.feature_dim = self._get_feature_dim_safely(img_size)

        # Global pooling
        self.global_pool = nn.AdaptiveAvgPool2d(1)

        # Classifier head
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(self.feature_dim, 1024),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate / 2),
            nn.Linear(1024, num_classes),
        )

    def _get_feature_dim_safely(self, img_size: int) -> int:
        """GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì•ˆì „í•œ feature dimension ê³„ì‚°"""
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, img_size, img_size)

            try:
                # GPUì—ì„œ ì‹œë„
                if torch.cuda.is_available():
                    features = self.backbone(dummy_input.cuda())
                    feature_dim = (
                        features.shape[1]
                        if len(features.shape) == 4
                        else features.shape[1]
                    )
                    # ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
                    del features, dummy_input
                    torch.cuda.empty_cache()
                else:
                    features = self.backbone(dummy_input)
                    feature_dim = (
                        features.shape[1]
                        if len(features.shape) == 4
                        else features.shape[1]
                    )
                    del features, dummy_input

                return feature_dim

            except RuntimeError as e:
                if "out of memory" in str(e):
                    print("âš ï¸ GPU OOM during feature dim calculation, using CPU")
                    torch.cuda.empty_cache()
                    # CPUì—ì„œ ê³„ì‚°
                    features = self.backbone(dummy_input.cpu())
                    feature_dim = (
                        features.shape[1]
                        if len(features.shape) == 4
                        else features.shape[1]
                    )
                    del features, dummy_input
                    gc.collect()
                    return feature_dim
                else:
                    raise e

    def forward(self, x):
        # Feature extraction
        features = self.backbone(x)

        # Global pooling if needed
        if len(features.shape) == 4:
            features = self.global_pool(features)
            features = features.flatten(1)

        # Classification
        output = self.classifier(features)
        return output


class Ensemble:
    """Soft Voting ì•™ìƒë¸” í´ë˜ìŠ¤"""

    def __init__(self, model_configs: List[Dict[str, Any]], device: torch.device):
        self.model_configs = model_configs
        self.device = device
        self.models = []
        self.weights = []

    def load_models(self, model_paths: List[str], num_classes: int):
        """ì €ì¥ëœ ëª¨ë¸ë“¤ì„ ë¡œë“œ"""
        assert len(model_paths) == len(self.model_configs), (
            "ëª¨ë¸ ê²½ë¡œì™€ ì„¤ì • ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        )

        for i, (config, model_path) in enumerate(zip(self.model_configs, model_paths)):
            print(f"Loading model {i + 1}: {config['name']}")

            try:
                # ëª¨ë¸ ìƒì„±
                model = Model(
                    num_classes=num_classes,
                    model_name=config["model_name"],
                    pretrained=False,  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œí•˜ë¯€ë¡œ False
                    dropout_rate=config["dropout_rate"],
                )

                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                checkpoint = torch.load(model_path, map_location=self.device)
                model.load_state_dict(checkpoint["model_state_dict"])
                model.to(self.device)
                model.eval()

                self.models.append(model)
                self.weights.append(config["weight"])

                print(f"âœ… {config['name']} loaded successfully")

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del checkpoint
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except Exception as e:
                print(f"âŒ Failed to load {config['name']}: {e}")
                raise e

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(self.weights)
        self.weights = [w / total_weight for w in self.weights]
        print(f"ğŸ“Š Ensemble weights: {[f'{w:.3f}' for w in self.weights]}")

    def predict(self, dataloader) -> np.ndarray:
        """ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰"""
        all_predictions = []

        print("ğŸ”® Performing ensemble prediction...")

        with torch.no_grad():
            for batch_idx, (images, _) in enumerate(dataloader):
                try:
                    images = images.to(self.device)
                    batch_predictions = []

                    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
                    for model in self.models:
                        outputs = model(images)
                        probs = torch.softmax(outputs, dim=1)
                        batch_predictions.append(probs.cpu().numpy())

                    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸”
                    weighted_pred = np.zeros_like(batch_predictions[0])
                    for pred, weight in zip(batch_predictions, self.weights):
                        weighted_pred += pred * weight

                    all_predictions.append(weighted_pred)

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del images, batch_predictions, weighted_pred
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    if batch_idx % 10 == 0:
                        print(f"  Processed batch {batch_idx + 1}/{len(dataloader)}")

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(
                            f"âš ï¸ OOM at batch {batch_idx}, clearing cache and retrying..."
                        )
                        torch.cuda.empty_cache()
                        gc.collect()
                        # ì¬ì‹œë„
                        continue
                    else:
                        raise e

        return np.vstack(all_predictions)

    def predict_single_model(self, dataloader, model_idx: int) -> np.ndarray:
        """ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡ (ê°œë³„ ì„±ëŠ¥ í™•ì¸ìš©)"""
        model = self.models[model_idx]
        all_predictions = []

        with torch.no_grad():
            for images, _ in dataloader:
                try:
                    images = images.to(self.device)
                    outputs = model(images)
                    probs = torch.softmax(outputs, dim=1)
                    all_predictions.append(probs.cpu().numpy())

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    del images, outputs, probs
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print("âš ï¸ OOM in single model prediction, clearing cache...")
                        torch.cuda.empty_cache()
                        gc.collect()
                        continue
                    else:
                        raise e

        return np.vstack(all_predictions)
