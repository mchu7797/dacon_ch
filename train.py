import os
import random
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
import torch
from torch.utils.data import DataLoader, Subset
from torchvision import transforms
from torch import nn, optim
from sklearn.metrics import log_loss, accuracy_score
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from typing import Tuple, Dict
import shutil
import gc
import psutil

from config import Config
from car_dataset import CarDataset
from model import Model


def set_seed(seed: int):
    """ì™„ì „í•œ ì‹œë“œ ê³ ì •"""
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    if torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def check_disk_space() -> bool:
    """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
    try:
        disk_usage = psutil.disk_usage(".")
        free_gb = disk_usage.free / (1024**3)
        if free_gb < 5:  # 5GB ë¯¸ë§Œì´ë©´ ê²½ê³ 
            print(f"âš ï¸ Warning: Low disk space ({free_gb:.1f}GB remaining)")
            return False
        return True
    except Exception as e:
        print(f"âŒ Failed to check disk space: {e}")
        return True


def get_transforms(
    config: Config,
) -> Tuple[transforms.Compose, transforms.Compose]:
    """ë°ì´í„° ì¦ê°• (ì•™ìƒë¸”ìš©ìœ¼ë¡œ ì¡°ì •)"""
    train_transform = transforms.Compose(
        [
            transforms.Resize((config.image_size + 32, config.image_size + 32)),
            transforms.RandomCrop(config.image_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),  # í° ëª¨ë¸ì´ë¯€ë¡œ ì•½ê°„ ê°ì†Œ
            transforms.ColorJitter(
                brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05
            ),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.1),
        ]
    )

    val_transform = transforms.Compose(
        [
            transforms.Resize((config.image_size, config.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    return train_transform, val_transform


class EarlyStopping:
    """Early stopping êµ¬í˜„"""

    def __init__(self, patience: int = 8, min_delta: float = 1e-4):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float("inf")
        self.counter = 0

    def __call__(self, val_loss: float) -> bool:
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device,
    epoch: int,
) -> float:
    """í•œ ì—í­ í›ˆë ¨"""
    model.train()
    total_loss = 0.0

    pbar = tqdm(train_loader, desc=f"Epoch {epoch} - Training")
    for batch_idx, (images, labels) in enumerate(pbar):
        try:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})

            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 50 ë°°ì¹˜ë§ˆë‹¤)
            if batch_idx % 50 == 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"âš ï¸ OOM at batch {batch_idx}, clearing cache and skipping...")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                continue
            else:
                raise e

    return total_loss / len(train_loader)


def validate_epoch(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device,
    num_classes: int,
) -> Dict[str, float]:
    """ê²€ì¦"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch_idx, (images, labels) in enumerate(
            tqdm(val_loader, desc="Validating")
        ):
            try:
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                loss = criterion(outputs, labels)
                total_loss += loss.item()

                probs = torch.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del images, labels, outputs, probs, preds
                if batch_idx % 20 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"âš ï¸ OOM during validation at batch {batch_idx}, skipping...")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    gc.collect()
                    continue
                else:
                    raise e

    avg_loss = total_loss / len(val_loader)
    accuracy = accuracy_score(all_labels, all_preds) * 100
    logloss = log_loss(all_labels, all_probs, labels=list(range(num_classes)))

    return {"loss": avg_loss, "accuracy": accuracy, "log_loss": logloss}


def train_single_model(
    model_config: Dict,
    fold: int,
    train_dataset: Subset,
    val_dataset: Subset,
    config: Config,
    num_classes: int,
    device: torch.device,
) -> str:
    """ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨"""

    print(f"\nğŸš€ Training {model_config['name']} - Fold {fold + 1}")

    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    if not check_disk_space():
        print("âš ï¸ Warning: Low disk space detected")

    # ë°ì´í„° ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    try:
        model = Model(
            num_classes=num_classes,
            model_name=model_config["model_name"],
            pretrained=model_config["pretrained"],
            dropout_rate=model_config["dropout_rate"],
            img_size=config.image_size,
        ).to(device)
    except RuntimeError as e:
        if "out of memory" in str(e):
            print("âŒ OOM during model initialization, reducing batch size...")
            config.batch_size = max(2, config.batch_size // 2)
            raise e
        else:
            raise e

    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
    )
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=8, T_mult=2, eta_min=1e-7)
    early_stopping = EarlyStopping(patience=config.patience, min_delta=config.min_delta)

    best_log_loss = float("inf")
    model_save_path = f"ensemble_{model_config['name']}_fold_{fold}.pth"

    # í›ˆë ¨ ë£¨í”„
    for epoch in range(1, config.num_epochs + 1):
        try:
            train_loss = train_epoch(
                model, train_loader, criterion, optimizer, device, epoch
            )
            val_metrics = validate_epoch(
                model, val_loader, criterion, device, num_classes
            )

            scheduler.step()

            print(
                f"  Epoch {epoch:2d}/{config.num_epochs} | "
                f"Train: {train_loss:.4f} | "
                f"Val: {val_metrics['loss']:.4f} | "
                f"Acc: {val_metrics['accuracy']:.2f}% | "
                f"LogLoss: {val_metrics['log_loss']:.4f}"
            )

            # ëª¨ë¸ ì €ì¥
            if val_metrics["log_loss"] < best_log_loss:
                best_log_loss = val_metrics["log_loss"]

                try:
                    torch.save(
                        {
                            "epoch": epoch,
                            "model_state_dict": model.state_dict(),
                            "optimizer_state_dict": optimizer.state_dict(),
                            "val_log_loss": best_log_loss,
                            "config": config,
                            "model_config": model_config,
                        },
                        model_save_path,
                    )
                    print(f"    ğŸ’¾ Best model saved! LogLoss: {best_log_loss:.4f}")
                except Exception as e:
                    print(f"âš ï¸ Failed to save model: {e}")

            # Early stopping
            if early_stopping(val_metrics["log_loss"]):
                print(f"    â¹ï¸ Early stopping at epoch {epoch}")
                break

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        except Exception as e:
            print(f"âŒ Error during epoch {epoch}: {e}")
            if "out of memory" in str(e):
                print("ğŸ’¡ Trying to continue with memory cleanup...")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                continue
            else:
                raise e

    print(
        f"âœ… {model_config['name']} training completed! Best LogLoss: {best_log_loss:.4f}"
    )
    return model_save_path


def main():
    config = Config()
    set_seed(config.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ Using device: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(
            f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB"
        )

    print(f"ğŸ“Š Ensemble models: {[m['name'] for m in config.ensemble_models]}")

    # ë°ì´í„° ì¤€ë¹„
    try:
        train_transform, val_transform = get_transforms(config)
        temp_dataset = CarDataset(config.train_root, transform=None)
        num_classes = len(temp_dataset.classes)
        targets = [label for _, label in temp_dataset.samples]
    except Exception as e:
        print(f"âŒ Failed to load dataset: {e}")
        return

    print(f"ğŸ“ Total samples: {len(temp_dataset)}, Classes: {num_classes}")

    # K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ê° ëª¨ë¸ í›ˆë ¨
    skf = StratifiedKFold(
        n_splits=config.n_folds, shuffle=True, random_state=config.seed
    )
    all_results = {}

    for model_config in config.ensemble_models:
        model_name = model_config["name"]
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ Training {model_name}")
        print(f"{'=' * 60}")

        fold_results = []
        model_paths = []

        for fold, (train_idx, val_idx) in enumerate(
            skf.split(range(len(temp_dataset)), targets)
        ):
            try:
                # ë°ì´í„°ì…‹ ìƒì„±
                train_dataset_full = CarDataset(
                    config.train_root, transform=train_transform
                )
                val_dataset_full = CarDataset(
                    config.train_root, transform=val_transform
                )

                train_dataset = Subset(train_dataset_full, train_idx)
                val_dataset = Subset(val_dataset_full, val_idx)

                # ëª¨ë¸ í›ˆë ¨
                model_path = train_single_model(
                    model_config,
                    fold,
                    train_dataset,
                    val_dataset,
                    config,
                    num_classes,
                    device,
                )

                # ê²°ê³¼ ê¸°ë¡
                try:
                    checkpoint = torch.load(model_path, map_location="cpu")
                    fold_results.append(checkpoint["val_log_loss"])
                    model_paths.append(model_path)
                    del checkpoint
                except Exception as e:
                    print(f"âš ï¸ Failed to load checkpoint for fold {fold}: {e}")
                    continue

                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del train_dataset, val_dataset, train_dataset_full, val_dataset_full
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()

            except Exception as e:
                print(f"âŒ Failed training fold {fold} for {model_name}: {e}")
                continue

        if fold_results:  # ì„±ê³µí•œ foldê°€ ìˆëŠ” ê²½ìš°ë§Œ
            # ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
            all_results[model_name] = {
                "fold_results": fold_results,
                "mean_score": np.mean(fold_results),
                "std_score": np.std(fold_results),
                "best_fold": np.argmin(fold_results),
                "model_paths": model_paths,
            }

            print(f"\nğŸ“ˆ {model_name} Results:")
            print(
                f"   Mean LogLoss: {np.mean(fold_results):.4f} Â± {np.std(fold_results):.4f}"
            )
            print(
                f"   Best Fold: {np.argmin(fold_results) + 1} (LogLoss: {min(fold_results):.4f})"
            )

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥ ë° ìµœê³  ëª¨ë¸ë“¤ ì„ íƒ
    if all_results:
        print(f"\n{'=' * 60}")
        print("ğŸ† FINAL ENSEMBLE RESULTS")
        print(f"{'=' * 60}")

        selected_models = []
        for model_name, results in all_results.items():
            best_fold = results["best_fold"]
            best_path = results["model_paths"][best_fold]
            final_path = f"ensemble_{model_name}_best.pth"

            try:
                shutil.copy(best_path, final_path)
                selected_models.append(final_path)

                print(f"ğŸ“Š {model_name}:")
                print(
                    f"   Mean: {results['mean_score']:.4f} Â± {results['std_score']:.4f}"
                )
                print(
                    f"   Best: {min(results['fold_results']):.4f} (Fold {best_fold + 1})"
                )
                print(f"   Model: {final_path}")
            except Exception as e:
                print(f"âš ï¸ Failed to copy best model for {model_name}: {e}")

        # ì•™ìƒë¸” ì •ë³´ ì €ì¥
        if selected_models:
            ensemble_info = {
                "models": [
                    config
                    for config in config.ensemble_models
                    if any(config["name"] in path for path in selected_models)
                ],
                "model_paths": selected_models,
                "results": all_results,
                "num_classes": num_classes,
                "class_names": temp_dataset.classes,
            }

            try:
                torch.save(ensemble_info, "ensemble_info.pth")
                print("\nğŸ’¾ Ensemble info saved: ensemble_info.pth")
                print("ğŸ¯ Ready for ensemble inference!")
            except Exception as e:
                print(f"âŒ Failed to save ensemble info: {e}")
        else:
            print("âŒ No models were successfully trained!")
    else:
        print("âŒ No models completed training successfully!")


if __name__ == "__main__":
    main()
