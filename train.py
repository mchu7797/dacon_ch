import os
import random
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
import torch
from torch.utils.data import DataLoader, Subset
from torchvision import transforms
from torch import nn, optim
from sklearn.metrics import log_loss, accuracy_score
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from typing import Tuple, Dict
import shutil

from config import Config
from car_dataset import CarDataset
from model import Model


def set_seed(seed: int):
    """ì™„ì „í•œ ì‹œë“œ ê³ ì •"""
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    if torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def get_transforms(
    config: Config,
) -> Tuple[transforms.Compose, transforms.Compose]:
    """ë°ì´í„° ì¦ê°• (ì•™ìƒë¸”ìš©ìœ¼ë¡œ ì¡°ì •)"""
    train_transform = transforms.Compose(
        [
            transforms.Resize((config.image_size + 32, config.image_size + 32)),
            transforms.RandomCrop(config.image_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),  # í° ëª¨ë¸ì´ë¯€ë¡œ ì•½ê°„ ê°ì†Œ
            transforms.ColorJitter(
                brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05
            ),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.1),
        ]
    )

    val_transform = transforms.Compose(
        [
            transforms.Resize((config.image_size, config.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    return train_transform, val_transform


class EarlyStopping:
    """Early stopping êµ¬í˜„"""

    def __init__(self, patience: int = 8, min_delta: float = 1e-4):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float("inf")
        self.counter = 0

    def __call__(self, val_loss: float) -> bool:
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device,
    epoch: int,
) -> float:
    """í•œ ì—í­ í›ˆë ¨"""
    model.train()
    total_loss = 0.0

    pbar = tqdm(train_loader, desc=f"Epoch {epoch} - Training")
    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += loss.item()
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / len(train_loader)


def validate_epoch(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device,
    num_classes: int,
) -> Dict[str, float]:
    """ê²€ì¦"""
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="Validating"):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    avg_loss = total_loss / len(val_loader)
    accuracy = accuracy_score(all_labels, all_preds) * 100
    logloss = log_loss(all_labels, all_probs, labels=list(range(num_classes)))

    return {"loss": avg_loss, "accuracy": accuracy, "log_loss": logloss}


def train_single_model(
    model_config: Dict,
    fold: int,
    train_dataset: Subset,
    val_dataset: Subset,
    config: Config,
    num_classes: int,
    device: torch.device,
) -> str:
    """ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨"""

    print(f"\nğŸš€ Training {model_config['name']} - Fold {fold + 1}")

    # ë°ì´í„° ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = Model(
        num_classes=num_classes,
        model_name=model_config["model_name"],
        pretrained=model_config["pretrained"],
        dropout_rate=model_config["dropout_rate"],
        img_size=config.image_size,
    ).to(device)

    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
    )
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=8, T_mult=2, eta_min=1e-7)
    early_stopping = EarlyStopping(patience=config.patience, min_delta=config.min_delta)

    best_log_loss = float("inf")
    model_save_path = f"ensemble_{model_config['name']}_fold_{fold}.pth"

    # í›ˆë ¨ ë£¨í”„
    for epoch in range(1, config.num_epochs + 1):
        train_loss = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )
        val_metrics = validate_epoch(model, val_loader, criterion, device, num_classes)

        scheduler.step()

        print(
            f"  Epoch {epoch:2d}/{config.num_epochs} | "
            f"Train: {train_loss:.4f} | "
            f"Val: {val_metrics['loss']:.4f} | "
            f"Acc: {val_metrics['accuracy']:.2f}% | "
            f"LogLoss: {val_metrics['log_loss']:.4f}"
        )

        # ëª¨ë¸ ì €ì¥
        if val_metrics["log_loss"] < best_log_loss:
            best_log_loss = val_metrics["log_loss"]
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "val_log_loss": best_log_loss,
                    "config": config,
                    "model_config": model_config,
                },
                model_save_path,
            )
            print(f"    ğŸ’¾ Best model saved! LogLoss: {best_log_loss:.4f}")

        # Early stopping
        if early_stopping(val_metrics["log_loss"]):
            print(f"    â¹ï¸ Early stopping at epoch {epoch}")
            break

    print(
        f"âœ… {model_config['name']} training completed! Best LogLoss: {best_log_loss:.4f}"
    )
    return model_save_path


def main():
    config = Config()
    set_seed(config.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ Using device: {device}")
    print(f"ğŸ“Š Ensemble models: {[m['name'] for m in config.ensemble_models]}")

    # ë°ì´í„° ì¤€ë¹„
    train_transform, val_transform = get_transforms(config)
    temp_dataset = CarDataset(config.train_root, transform=None)
    num_classes = len(temp_dataset.classes)
    targets = [label for _, label in temp_dataset.samples]

    print(f"ğŸ“ Total samples: {len(temp_dataset)}, Classes: {num_classes}")

    # K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ê° ëª¨ë¸ í›ˆë ¨
    skf = StratifiedKFold(
        n_splits=config.n_folds, shuffle=True, random_state=config.seed
    )
    all_results = {}

    for model_config in config.ensemble_models:
        model_name = model_config["name"]
        print(f"\n{'=' * 60}")
        print(f"ğŸ¯ Training {model_name}")
        print(f"{'=' * 60}")

        fold_results = []
        model_paths = []

        for fold, (train_idx, val_idx) in enumerate(
            skf.split(range(len(temp_dataset)), targets)
        ):
            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset_full = CarDataset(
                config.train_root, transform=train_transform
            )
            val_dataset_full = CarDataset(config.train_root, transform=val_transform)

            train_dataset = Subset(train_dataset_full, train_idx)
            val_dataset = Subset(val_dataset_full, val_idx)

            # ëª¨ë¸ í›ˆë ¨
            model_path = train_single_model(
                model_config,
                fold,
                train_dataset,
                val_dataset,
                config,
                num_classes,
                device,
            )

            # ê²°ê³¼ ê¸°ë¡
            checkpoint = torch.load(model_path, map_location="cpu")
            fold_results.append(checkpoint["val_log_loss"])
            model_paths.append(model_path)

        # ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
        all_results[model_name] = {
            "fold_results": fold_results,
            "mean_score": np.mean(fold_results),
            "std_score": np.std(fold_results),
            "best_fold": np.argmin(fold_results),
            "model_paths": model_paths,
        }

        print(f"\nğŸ“ˆ {model_name} Results:")
        print(
            f"   Mean LogLoss: {np.mean(fold_results):.4f} Â± {np.std(fold_results):.4f}"
        )
        print(
            f"   Best Fold: {np.argmin(fold_results) + 1} (LogLoss: {min(fold_results):.4f})"
        )

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥ ë° ìµœê³  ëª¨ë¸ë“¤ ì„ íƒ
    print(f"\n{'=' * 60}")
    print("ğŸ† FINAL ENSEMBLE RESULTS")
    print(f"{'=' * 60}")

    selected_models = []
    for model_name, results in all_results.items():
        best_fold = results["best_fold"]
        best_path = results["model_paths"][best_fold]
        final_path = f"ensemble_{model_name}_best.pth"

        shutil.copy(best_path, final_path)
        selected_models.append(final_path)

        print(f"ğŸ“Š {model_name}:")
        print(f"   Mean: {results['mean_score']:.4f} Â± {results['std_score']:.4f}")
        print(f"   Best: {min(results['fold_results']):.4f} (Fold {best_fold + 1})")
        print(f"   Model: {final_path}")

    # ì•™ìƒë¸” ì •ë³´ ì €ì¥
    ensemble_info = {
        "models": config.ensemble_models,
        "model_paths": selected_models,
        "results": all_results,
        "num_classes": num_classes,
        "class_names": temp_dataset.classes,
    }

    torch.save(ensemble_info, "ensemble_info.pth")
    print("\nğŸ’¾ Ensemble info saved: ensemble_info.pth")
    print("ğŸ¯ Ready for ensemble inference!")


if __name__ == "__main__":
    main()
